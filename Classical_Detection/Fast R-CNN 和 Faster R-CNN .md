# Fast R-CNN 和 Faster R-CNN 

**出处会议：** Fast R-CNN: ICCV 2015 / Faster R-CNN: NeurIPS 2015  
**是否开源：** 是，https://github.com/rbgirshick/py-faster-rcnn  
**关键词：** RoI Pooling、候选区域网络(RPN)、锚框、多任务损失

---

## Fast R-CNN

**核心贡献：** 解决了 R-CNN 训练步骤繁琐、测试速度慢的问题，提出了单阶段训练 Single-stage training 和 RoI Pooling。

### 方法

#### 整体架构

- **输入：** 接收整张图片和一组候选框，依然由外部算法如 Selective Search 生成。
- **共享卷积特征 (Shared Convolutions)：** 整张图片只经过一次 CNN 前向传播生成特征图（Feature Map），不再像 R-CNN 那样对每个候选框单独计算卷积，极大地减少了计算量。
- **RoI Pooling 层：** 它将特征图上不同大小的候选区域（Region of Interest, RoI）映射并池化为固定大小的特征向量（例如 $7 \times 7$），以便输入全连接层。
- **输出层：** 全连接层后分为两个分支：
  1. **分类器 (Softmax)：** 输出 $K+1$ 个类别的概率（含背景）4。
  2. **回归器 (Bbox Regressor)：** 输出 $K$ 个类别的边界框偏移量 $(dx, dy, dw, dh)$ 。

---

具体的，**RoI Pooling** 的过程可以清晰地拆解为两个核心步骤：**映射（定义窗口）** 和 **池化（网格划分与取最大值）**。

其目的是将特征图上大小不一的 RoI 窗口（$h \times w$），强行统一成网络预设的固定尺寸 $H \times W$（例如 $7 \times 7$）。



映射，将候选框（RoI）定位到卷积特征图上。

- **坐标表示：** 每个 RoI 由一个四元组 $(r, c, h, w)$ 定义，分别代表其**左上角坐标** $(r, c)$ 以及它的**高度和宽度** $(h, w)$ 。

- **坐标转换：** 虽然 RoI 最初是在原始图像上生成的（例如通过 Selective Search），但在进行 RoI Pooling 之前，它会被映射到特征图的坐标系中。

*注意：* 这一步通常涉及将原图坐标除以网络的**步长 (Stride)**。例如，如果 VGG16 的步长是 16（即特征图比原图缩小了 16 倍），那么原图上的坐标和尺寸都需要除以 16 并取整，才能得到特征图上的 $(r, c, h, w)$ 。



一旦在特征图上确定了 $h \times w$ 的窗口，就需要通过以下机制将其转化为固定的 $H \times W$ 输出：

- 网格划分 (Gridding)：

  将特征图上那个 $h \times w$ 的 RoI 窗口，切割成一个 $H \times W$ 的网格 。

  - **子窗口大小：** 每个网格单元（子窗口）的大小大约是 $h/H \times w/W$ 6。
  - *例子：* 假设 RoI 在特征图上的大小是 $21 \times 14$，目标输出大小是 $7 \times 7$。那么它会被切成 $7 \times 7$ 个格子，每个格子（子窗口）的大小就是 $3 \times 2$ 像素。

- 最大池化 (Max Pooling)：

  在切好的每一个子窗口内，执行标准的最大池化操作 7。

  - 也就是取出该子窗口内所有像素值的**最大值**，填入输出网格对应的位置。

- 通道独立 (Channel Independence)：

  上述操作是对特征图的每一个通道（Channel）独立进行的 。

  - 如果特征图有 512 个通道（如 VGG16），那么这个过程会重复 512 次，最终输出一个 $512 \times H \times W$ 的数据块。



正是通过这种“**切分网格 -> 局部取最大**”的机制，RoI Pooling 成功地把任意尺寸的候选框都变成了固定大小的特征向量，从而实现了 Fast R-CNN 的端到端训练。

---

#### **损失函数**

Fast R-CNN 将分类和回归的训练合并为一个损失函数，实现联合优化：
$$
L(p, u, t^u, v) = L_{cls}(p, u) + \lambda [u \ge 1] L_{loc}(t^u, v)
$$


 $$L_{cls}$$ ：分类损失部分，负责判断 这是什么。

- **输入 ($p$)：** 经过 Softmax 层输出的概率分布 $p = (p_0, ..., p_K)$，对应 $K+1$ 个类别（K 个物体类 + 1 个背景类）。

- **真值 ($u$)：** 该候选框对应的真实类别标签（ground-truth class）。

- 计算方式： 使用标准的对数损失（Log Loss）：
  $$
  L_{cls}(p, u) = -\log p_u
  $$
  即对于真实类别 $u$，我们希望其预测概率 $p_u$ 越大越好（损失越小越好）。

  

$$L_{loc}$$ ： 定位（回归）损失部分，负责判断 框应该怎么修才更准。

- **开关 ($[u \ge 1]$)：** 

  - 当 $u \ge 1$，即候选框是物体时，值为 1，计算回归损失。
  - 当 $u = 0$，即候选框是背景时，值为 0，**忽略**回归损失。

- **输入 ($t^u$)：** 网络预测的坐标偏移量 $t^u = (t^u_x, t^u_y, t^u_w, t^u_h)$。注意这里是 $t^u$，意味着网络为每一个类别都专门预测了一组坐标偏移。

- **真值 ($v$)：** 真实边框相对于候选框的回归目标。

- 计算方式：这是 Fast R-CNN 的一个重要改进。使用了 $$\text{smooth}_{L1}$$ Loss：
  $$
  L_{loc}(t^u, v) = \sum_{i \in \{x,y,w,h\}} \text{smooth}_{L1}(t^u_i - v_i)
  $$
  其中 $\text{smooth}_{L1}$ 定义为：
  $$
  \text{smooth}_{L1}(x) = \begin{cases} 0.5x^2 & \text{if } |x| < 1 \\ |x| - 0.5 & \text{otherwise} \end{cases}
  $$

Fast R-CNN 论文指出：

- L2 Loss 对离群点过于敏感，训练时容易发生**梯度爆炸 (Exploding Gradients)**。
- L1 Loss 在误差非常小的时候，它的函数图像是一个尖角（V字形），不可导。这意味着当预测已经很准的时候，它容易在目标附近“震荡”，**很难收敛**到精确的 0 点。

所以， **Fast R-CNN** 为了解决上述两个问题，它结合了 L1 和 L2 的优点。

**设计思路：**

- 当误差很大时（像 L1）：用线性函数，防止梯度爆炸。
- 当误差很小时（像 L2）：用平方函数，变得 Smooth ，方便收敛。

此外 $\lambda$ 是一个超参数，用于控制分类损失和回归损失之间的平衡。



多任务损失函数使得 Fast R-CNN 能够通过一次反向传播，同时更新分类器和边界框回归器的参数，实现了真正的**单阶段训练 (Single-stage training)**。

---

### 消融实验

#### 实验一：多任务训练真的有效吗？(Does multi-task training help?)

Fast R-CNN 最大的特点是不再分阶段训练分类和回归，而是用一个损失函数一起练。

- **对比设置：**
  1. **基准线 (Baseline):** 仅使用分类损失 ($L_{cls}$) 训练网络，没有回归层。
  2. **多任务训练 (Multi-task):** 同时训练分类和回归。
  3. **分阶段训练 (Stage-wise):** 先练分类，固定参数后再练回归（模拟 R-CNN 的流程）。
- **结果 (Table 6)：对于 S, M, L 三个模型，**多任务训练**都比**仅训练分类要好。

#### 实验二：尺度不变性 (Scale Invariance) 

目标检测需要处理不同大小的物体。是应该把图片缩放成各种大小（金字塔）输入网络，还是只用一个大小（暴力学习）？

- **对比设置：**
  1. **单尺度 (Single-scale, 暴力学习):** 训练和测试时，图片短边固定为 600 像素 8888。
  2. **多尺度 (Multi-scale, 图像金字塔):** 训练和测试时，图片在 $\{480, 576, 688, 864, 1200\}$ 中随机采样或遍历。
- **结果 (Table 7)：**
  - **单尺度策略表现惊人地好。** 深度网络（特别是 S, M, L）非常擅长直接学习尺度不变性。
  - 多尺度虽然能稍微提升一点点 mAP（例如模型 M 从 59.2% 提升到 60.7%），但计算时间却大幅增加。
  - **结论：** 为了速度和精度的最佳平衡，Fast R-CNN 最终推荐使用**单尺度** ($s=600$)。

#### 实验三：SVM 还是 Softmax？(Do SVMs outperform softmax?)

R-CNN 的经典流程是“CNN 提特征 -> SVM 分类”。Fast R-CNN 试图证明直接用神经网络自带的 Softmax 就够了。

- **对比设置：**
  1. **Softmax:** Fast R-CNN 的标准输出。
  2. **SVM:** 在 Fast R-CNN 训练完后，固定特征，像 R-CNN 那样训练一堆 SVM（带难例挖掘）。
- **结果 (Table 8)：**
  - **Softmax 略微优于 SVM。** 三个模型 (S, M, L) 使用 Softmax 的 mAP 都比 SVM 高出 0.1 到 0.8 个点 12。
  - 这证明了“一步到位”的微调（Fine-tuning）比旧的多阶段训练更强，而且省去了训练 SVM 和磁盘读写特征的麻烦 13。

#### 实验四：候选框越多越好吗？(Are more proposals always better?)

- **对比设置：**
  - 使用 Selective Search 生成不同数量的框（1k ~ 10k）。
  - 使用密集滑动窗口（Dense boxes，约 45k 个）。
- **结果 (Figure 3)：**
  - **不是越多越好。** 随着候选框数量增加，mAP 并没有一直上升，反而可能下降 14。
  - **稀疏优于密集。** Selective Search（稀疏框）的效果远好于密集滑动窗口（Dense boxes）。如果单纯给分类器塞入大量密集的框（45k个），准确率甚至不如只用 2000 个 Selective Search 框。
  - 这表明候选框机制不仅仅是为了加速，还起到了一种高质量的**级联过滤 (Cascade)** 作用，帮助分类器排除简单背景。

这些消融实验有力地支持了 Fast R-CNN 的最终设计：

1. **用 VGG16 (Model L) 最准。**
2. **用多任务损失联合训练。**
3. **用单尺度图片输入（$s=600$）以换取速度。**
4. **直接用 Softmax 分类，抛弃 SVM。**


------

## Faster R-CNN

**问题**：Fast R-CNN 虽然快，但依赖 CPU 运行的 Selective Search 生成候选框，这步耗时约 2秒/张，成为推理速度的瓶颈。

**核心贡献：** 引入 **RPN (Region Proposal Network)**，替代了慢速的 Selective Search，实现了真正的端到端实时检测。

---

### 方法

**原理：** RPN 的核心任务是：“在特征图上滑动，告诉我们哪里有物体”。

---

在滑动窗口的每一步，网络凭空预测坐标是非常困难的。RPN 引入了 **Anchors** 作为参照物。

**定义**：在特征图上的每一个滑动窗口中心点，RPN 都会预先生成 $k$ 个固定大小和形状的矩形框，这些框就叫 **Anchors** 

RPN 不直接预测坐标，而是基于预设的 Anchors 进行修正。在每个滑动位置，默认设置 3 种尺度 ($128^2, 256^2, 512^2$) 和 3 种长宽比 (1:1, 1:2, 2:1)，共 $k=9$ 个 Anchors。

- **优势：** 这种设计无需多次卷积，也不需要放缩图片，即可检测多尺度物体。

- **双分支输出：** RPN 输出每个 Anchor  是物体还是背景 和 边界框回归偏移量。
  - **cls layer (分类层)：** 预测该位置是否包含物体。**输出**：$2k$ 个数值，每个 Anchor 有“是物体”和“不是物体” 。
  - **reg layer (回归层)：** 预测候选框的坐标修正值。**输出**：$4k$ 个数值，每个 Anchor 有 4 个坐标修正值 $x, y, w, h$。

------

整体框架：

- 输入： 接收来自主干网络（如 VGG16）的共享卷积特征图（Feature Map）。

- RPN 使用一个小网络，通常是 $3 \times 3$ 的卷积核，在特征图上进行滑动扫描，每个滑动位置会被映射为一个低维特征向量。
- 这个特征向量会被送入两个同级（Sibling）的全连接层，然后输出。



------

**损失函数**

简单来说，RPN 的训练就是让网络根据每一个 Anchor 与真实框（Ground Truth）的重叠程度，去学习两件事：**“它是物体吗？”** 和 **“它离真实框偏了多少？”**。

----

**给 Anchors 打标签**

在计算损失之前，网络首先要知道哪些 Anchor 是“对的”（正样本），哪些是“错的”（负样本）。

这是通过计算 Anchor 与真实框的 **IoU (交并比)** 来决定的：

- **正样本 (Positive Label, $p^*=1$)：**
  - 与某个真实框 IoU 最高的 Anchor。或者，与任意真实框的 IoU 超过 0.7 的 Anchor。
  - 应该被预测为“物体”，且需要进行位置修正。
- **负样本 (Negative Label, $p^*=0$)：**
  - 与所有真实框的 IoU 都低于 0.3 的 Anchor。
  - 应该被预测为“背景”，不需要位置修正。
- **忽略样本 (Ignored)：**
  - IoU 在 0.3 到 0.7 之间的 Anchor。
  - 既不当正样本也不当负样本，**不参与损失计算**。

---

**构建多任务损失函数**

一旦每个 Anchor 有了标签，就可以代入损失函数进行计算。RPN 的总损失是 分类损失 和 回归损失 的加权和：
$$
L(\{p_{i}\},\{t_{i}\}) = \frac{1}{N_{cls}}\sum_{i}L_{cls}(p_{i},p_{i}^{*}) + \lambda\frac{1}{N_{reg}}\sum_{i}p_{i}^{*}L_{reg}(t_{i},t_{i}^{*})
$$
这个公式里有两个关键部分：

**$$L_{cls}$$ 分类损失**

- **作用对象：** 所有的正样本和负样本。

- **计算：** 使用二分类对数损失 (Log Loss)。

- **目的：** 如果 Anchor 是正样本，强迫网络输出高概率；如果是负样本，强迫网络输出低概率。



**$$L_{reg}$$ 回归损失**

- **作用对象：** 公式里的 $p_i^*$ 项，使其**仅针对正样本**。

- **计算：** 使用 **Smooth L1 Loss** 计算预测偏移量 $t_i$ 和真实偏移量 $t_i^*$ 之间的差距。

它学习的是这 4 个变换参数：

- $t_x = (x - x_a) / w_a$  (中心点 $x$ 需要平移多少个宽度的比例)
- $t_y = (y - y_a) / h_a$  (中心点 $y$ 需要平移多少个高度的比例)
- $t_w = \log(w / w_a)$    (宽度需要缩放多少倍，取对数)
- $t_h = \log(h / h_a)$    (高度需要缩放多少倍，取对数)

其中 $x, x_a$ 分别代表预测框、Anchor 框和真实框的坐标。

---

RPN 是这样结合损失学习的：

1. **分工：** 用 IoU 把 Anchors 分为正负样本。
2. **筛选：** 用 $p_i^*$ 确保只有正样本参与回归学习，负样本只学分类。
3. **相对回归：** 结合 Smooth L1 Loss，强迫网络学习**相对于 Anchor 的几何变换参数**，而不是绝对坐标。

---

### 消融实验

#### 实验一：共享特征的重要性

验证：**让 RPN 和 Fast R-CNN 共享卷积层，是否真的比独立训练两个网络更好？**

- **实验设置：**

  - **不共享 (Unshared):** 训练 RPN，生成 Proposal，然后独立训练 Fast R-CNN（就像之前的 SS + Fast R-CNN 流程一样）。
  - **共享 (Shared):** 使用论文提出的 4步交替训练法，让两个网络共享卷积层。

- **结果 (Table 2):**

  **不共享：** mAP 为 58.7%。

  - **共享：** mAP 提升至 **59.9%**。

- **结论：** 共享特征不仅实现了更高效的计算，不用算两次卷积；而且提升了精度。这是因为检测器微调后的特征反过来也优化了 RPN 生成 Proposal 的质量。

### 实验二：RPN 分支的作用 (cls & reg)

RPN 有两个输出分支：**分类 (cls)** 和 **回归 (reg)**。这一实验测试这两个分支是否缺一不可。

- **测试移除分类分支 (No cls):**
  - **操作：** 移除得分预测，导致无法进行 NMS（非极大值抑制）或排序。只能从所有 Proposal 中**随机**抽取 N 个输入检测器。
  - **结果：** 当 N=1000 时，mAP 几乎没变（55.8%）；但当 **N=100** 时，mAP 暴跌至 **44.6%**。
  - **结论：** `cls` 分数对于筛选高质量的 Proposal 至关重要，尤其是在 Proposal 数量较少时，它保证了我们选的是“像物体”的框 2。
- **测试移除回归分支 (No reg):**
  - **操作：** 移除坐标修正，直接把原始的 **Anchors** 当作 Proposal 输入检测器。
  - **结果：** mAP 跌至 **52.1%**。
  - **结论：** 原始的 Anchor 还是太粗糙了，不足以作为精确的候选框。`reg` 分支对于生成高质量 Proposal 是核心贡献者 3。

### 实验三：Anchors 的设计 (Scales & Aspect Ratios)

这一实验验证了为什么要用 3 种尺度和 3 种长宽比（共 9 个 Anchors）。

- **实验设置 (Table 8):**

  - **1 尺度 + 1 比例 (1 Anchor):** mAP 只有 65.8% - 66.7%。
  - **1 尺度 + 3 比例 (3 Anchors):** mAP 提升至 68.8%。
  - **3 尺度 + 1 比例 (3 Anchors):** mAP 提升至 69.8%。
  - **3 尺度 + 3 比例 (9 Anchors - 默认):** mAP 为 **69.9%**。

- **结论：**

  - 只用一种 Anchor 会导致精度大幅下降（3-4%）。

  - 使用多尺度或多比例均可提升性能。

  - 虽然 3 尺度 + 1 比例的效果已经和完整版差不多（69.8% vs 69.9%），但为了系统的灵活性，作者保留了 3x3 的设计 4。

    

### 实验四：候选框数量与 NMS 的影响

- **NMS 的作用：**

  - RPN 原始输出约 6000 个框。如果不做 NMS 直接用，mAP 为 55.2%。
  - 做了 NMS 后剩下 300 个框，mAP 为 56.8%。
  - **结论：** NMS 没有降低精度，反而可能减少了误报，且极大减少了后续计算量 5。

- **Top-N 的数量：**

  - 使用前 300 个 Proposal 和前 1000 个 Proposal 的效果差不多。这说明 RPN 能够把正确的框排在很靠前的位置，**300 个框就足够了** 。

    

### 实验五：单阶段检测 vs 两阶段检测 (One-Stage vs Two-Stage)

这是为了回应对 OverFeat 等早期单阶段方法的质疑：**“一定要先生成框再检测吗？直接在特征图上密集预测不行吗？”**

- **实验设置 (Table 10):**
  - **Two-Stage (Faster R-CNN):** RPN 提议 -> Fast R-CNN 检测。
  - **One-Stage (模拟 OverFeat):** 移除了 Proposal 步骤，直接在特征图上使用密集的滑动窗口进行分类和回归。
- **结果：**
  - **Two-Stage:** mAP 58.7%。
  - **One-Stage:** mAP 53.9%。
- **结论：** 两阶段方法比单阶段高出 **4.8%**。这证明了“区域建议 (Region Proposal)”不仅仅是为了加速，它还起到了**级联过滤 (Cascade)** 的作用，帮助剔除大量简单的背景负样本，让检测器专注于更难的目标 7。

### 实验六：超参数 $\lambda$ 的敏感性

- **内容：** 损失函数中平衡分类和回归权重的参数 $\lambda$。

- **结果 (Table 9):** $\lambda$ 取 1, 10, 100 时，mAP 变化很小（约 1%）。

- **结论：** 结果对 $\lambda$ 不敏感，默认取 10 是合理的 8。

  

这些消融实验有力地证明了 Faster R-CNN 核心设计的正确性：

1. **RPN 必须有：** 它的 `cls` 和 `reg` 缺一不可。
2. **特征必须共享：** 联合训练提升了两个任务的表现。
3. **两阶段优于单阶段：** 先提议再检测的策略在当时是精度最优解。
4. **Anchors 机制有效：** 多尺度的 Anchors 解决了多尺度物体检测的难题。
