# EMC2: 基于边缘混合专家的自动驾驶 3D 目标检测系统

**出处会议：** AAAI 2025  
**是否开源：** 没有  
**关键词：** 混合专家模型、边缘计算、多模态融合、自适应调度

---

## 1. 概述

在自动驾驶系统（ADS）中，感知模块面临**高精度**与**低延迟**的固有矛盾。

**EMC2 (Edge-based Mixture of Experts Collaborative Computing)** 提出了一种算法-硬件-软件协同优化的方案，通过动态激活异构专家模型，在边缘端实现了精度与效率的平衡。

能在 边缘平台，**如 Jetson AGX Orin**上实时运行。

------

## 2. 系统架构组件

### 2.1 自适应多模态数据桥 (AMDB)

AMDB 是系统的预处理与分发中心。

- **常驻 的 LiDAR 分支**：
  - 使用 UNet 和轻量化 3D CNN 提取点云特征。
  - **Proposal Regions 生成**：产生初步的候选区域及其置信度分数 $C$。
- **动态激活 的 图像分支**：
  - 仅在场景自适应调度器（SAD）判定为“困难场景”时，才会启动 ResNet 提取 2D 图像特征。
- **多尺度池化 (Multiscale Pooling, MP)**：
  - **背景**：边缘设备（如 Jetson）的 L1/L2 缓存极小，无法容纳完整的稠密图像特征。
  - **实现**：根据设备当前可用显存，动态调整池化核大小，对投影到 3D 空间的图像特征进行压缩。
  - **对齐**：将池化后的图像特征与对应的 LiDAR Voxel 特征拼接（Concat），形成多模态特征包分发给 APE 专家。

### 2.2 场景自适应调度器 (SAD)

SAD 是决策核心，根据 AMDB 提供的目标距离 $D$ 和置信度 $C$ 决定推理路径。

**路由判据与逻辑：**

1. **LPE (延迟优先) 路径**：
   - **判定条件**：所有候选区域满足 $\text{dist} < D$ 且 $\text{conf} \ge C$。
   - **逻辑**：近场且清晰，直接使用 2D CNN 处理 BEV 投影，极致压低延迟。
2. **VEE (通用效率) 路径**：
   - **判定条件**：存在 $\text{dist} < D$ 但 $\text{conf} < C$（近但模糊），或 $\text{dist} \ge D$ 但 $\text{conf} \ge C$（远但清晰）。
   - **逻辑**：升级为 3D 稀疏卷积，保留空间几何结构。
3. **APE (精度优先) 路径**：
   - **判定条件**：存在 $\text{dist} \ge D$ 且 $\text{conf} < C$。
   - **逻辑**：最难场景（远且模糊），强制开启图像融合路径，靠纹理语义补偿几何缺失。

### 2.3 三类异构专家

- **LPE (Latency-Prioritized Expert)**：核心为 **2D CNN**。将 3D 建议框投影至 BEV（俯视图）空间处理，完全避开耗时的 3D 卷积。
- **VEE (Versatile Efficiency Expert)**：核心为 **3D 稀疏卷积**。在体素空间进行编码解码，保留完整的 3D 空间结构。
- **APE (Accuracy-Prioritized Expert)**：核心为 **多模态融合 + 3D 稀疏卷积**。将图像 ResNet 特征投影并与 LiDAR Voxel 拼接，实现特征级融合。

------

## 3. 核心数学公式与算法

### 3.1 层次化训练 (Hierarchical Training)

**各路由损失函数：**
$$
\mathcal{L}_{route} = \mathcal{L}_{cls} + \mathcal{L}_{reg}
$$


**分类损失 ($\mathcal{L}_{cls}$)：** 采用交叉熵
$$
\mathcal{L}_{cls} = - \sum y \log(p)
$$

**回归损失 ($\mathcal{L}_{reg}$)：** 采用 Smooth-L1 监督 7 个自由度（$x, y, z, w, l, h, \theta$）
$$
\text{Smooth-L1}(x) = \begin{cases} 0.5x^2, & \text{if } |x| < 1 \\ |x| - 0.5, & \text{otherwise} \end{cases}
$$

#### 第一阶段：预训练分流 (Pre-training Flow)

- AMDB 的 LiDAR 分支和图像分支首先**独立进行 20 个 epoch 的预训练** 。此时梯度仅在各自分支内部回传，确立稳定的底层特征表达。

#### 第二阶段：联合训练流 (Joint Training Flow)

- **流动起点**：为了避免重叠训练数据导致的频繁重复更新（反复震荡），联合优化时，梯度**仅从 APE（精度优先专家）向上传导**至 AMDB 。
- **逻辑**：APE 接收完整的多模态输入，其梯度能同时更新 LiDAR 和图像两个分支，从而在稳定状态下微调整个系统。

### 3.2 长尾效应解决方案

针对 MoE 中极端场景专家训练不足的问题：

- **平衡采样策略**：

  确保各专家子数据集的选择概率 $\mathcal{P}_i$ 与样本量 $\mathcal{N}_{\mathcal{S}_i}$ 满足：
  $$
  \sum_{i=1}^{3} \mathcal{P}_i \times \mathcal{N}_{\mathcal{S}_i} = 1
  $$

- **自适应优化器 (Adaptive Optimizer)**：

  根据 Batch 内目标样本（对口样本）的占比 $p$ 动态调整学习率 $\alpha$：
  $$
  \alpha = (1 + p) \cdot \alpha_0
  $$
  

  其中 $p = \frac{1}{\mathcal{N}}\sum_{i=1}^{\mathcal{N}}1_{i\in\mathcal{T}}$，$\alpha_0$​ 为基础学习率。$\mathcal{N}$ 为Batch 大小。$\mathcal{T}$ 为目标样本子集。当 Batch 中专业对口的样本比例越高，学习率越大，强制专家在擅长领域快速精进。

### 3.3 算子复杂度优化

传统 3D 卷积复杂度为 $O(\mathcal{H}^3 \times \mathcal{C}_v)$，EMC2 算子仅处理非空体素 $\mathcal{N}$，复杂度降至 $O(\mathcal{N} \times \mathcal{C}_v)$，减少了约 **65-80%** 的冗余计算。

**并行搜索**：利用 GPU **并行前缀和（Parallel Prefix-sum）** 算法，将体素索引搜索时间从 $O(\mathcal{N})$ 优化至 $O(\log \mathcal{N})$。

------

## 4. 系统级优化手段

1. **线程管理**：设置 4 个线程终止边界，跨越边界时立即释放上一阶段的线程地址空间，降低显存峰值。
2. **计算图融合**：在 ONNX 层级合并连续算子（如 Conv+ReLU），减少显存回写与内核启动开销。
3. **通信重叠 (Overlap)**：将权重矩阵切片，在 GPU 计算当前块时，利用 DMA 提前搬运下一块数据。

------

## 5. 实验分析

### 5.1 KITTI 数据集性能对比 (Table 1)

| **方法 (Method)** | **行人 3D AP (Hard)** | **汽车 3D AP (Hard)** | **骑行者 3D AP (Hard)** | **Jetson 延迟 (ms)** |
| ----------------- | --------------------- | --------------------- | ----------------------- | -------------------- |
| PointPillar       | 46.87                 | 75.19                 | 58.97                   | 41                   |
| Second-IoU        | 49.50                 | 79.62                 | 71.23                   | 58                   |
| LoGoNet           | 59.46                 | 82.92                 | 72.42                   | 965                  |
| TED               | 63.63                 | 88.94                 | 71.59                   | 154 (A4000)          |
| **EMC2 (Ours)**   | **66.81**             | **88.15**             | **77.62**               | **372.5**            |

**分析**：EMC2 在 Hard 难度下显著优于主流方法。虽然纯点云模型（如 PointPillar）极快，但精度无法支撑安全需求；而 LoGoNet 等多模态模型在 Jetson 上延迟高达 965ms，EMC2 成功将延迟控制在 400ms 以内并保持了顶尖精度。

### 5.2 nuScenes 数据集性能对比 (Table 2)

| **方法 (Method)** | **参数量 (Size)** | **mAP**    | **NDS**    | **Jetson 延迟 (ms)** |
| ----------------- | ----------------- | ---------- | ---------- | -------------------- |
| FocalFormer       | 189M              | 0.6640     | 0.7090     | N/A                  |
| BEVFusion         | 156M              | 0.6852     | 0.7138     | N/A                  |
| **EMC2 (Ours)**   | **87M (Active)**  | **0.7241** | **0.7316** | **229.3**            |

**分析**：EMC2 通过 MoE 架构，在推理时平均仅激活 87M 参数，mAP 相比 BEVFusion 提升了约 3.9%，证明了动态子网络选择的优越性。

### 5.3 消融实验分析 (Table 3)

| **消融配置 (Ablation)** | **专家/方案**      | **行人 AP** | **汽车 AP** | **延迟 (ms)** |
| ----------------------- | ------------------ | ----------- | ----------- | ------------- |
| **针对 LPE 场景**       | LPE                | 66.32       | 88.54       | **219**       |
| (近场清晰场景)          | VEE                | 74.49       | 91.69       | 468           |
|                         | APE                | 74.34       | 92.41       | 526           |
| **针对 VEE 必要性**     | **W/ VEE**         | 70.55       | 84.07       | **372.5**     |
| (全量场景测试)          | **W/O VEE**        | 68.26       | 86.40       | 512.7         |
| **针对 APE 场景**       | VEE (LiDAR Only)   | 67.24       | 81.85       | 468           |
| (远场模糊场景)          | APE (Multimodal)   | **70.55**   | **84.18**   | 526           |
| **组合策略验证**        | **HT & MP (开启)** | 70.55       | 84.07       | **526**       |
|                         | 仅开启 HT          | 70.55       | 84.18       | 1320          |
|                         | 仅开启 MP          | 63.11       | 75.98       | 529           |

**深度分析总结：**

1. **针对 LPE**：在近场场景，LPE 的精度与强力专家 APE 极其接近，但延迟仅为其 **40%**，证明了异构分流的正确性。
2. **针对 VEE**：**W/ (With)** 代表有 VEE，**W/O (Without)** 代表无 VEE。当去掉中层的 VEE，系统被迫将大量场景交给 APE，导致系统总延迟从 372.5ms 飙升至 512.7ms。
3. **针对 APE**：在远场极限场景下，APE 相比纯点云的 VEE 提升了约 3.3% 的行人精度，证明了图像融合在极端场景的救命作用。
4. **HT & MP**：**HT（层次化训练）** 是精度的保险柜，禁用它精度会掉 8% 以上；**MP（多尺度池化）** 是速度的加速器，禁用它延迟会增加 **150%**。
